{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jessicantma/Task_Week4/blob/main/taskweek4_jessicanatamanapitupulu_2106726150.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jessica Natama Napitupulu 2106726150"
      ],
      "metadata": {
        "id": "M8yYLgCGkgMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install d2l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cw85vmAQ_9W",
        "outputId": "786b38b5-df66-4d46-8d74-38c86d92b1c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: d2l in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.10/dist-packages (from d2l) (1.0.0)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (from d2l) (1.23.5)\n",
            "Requirement already satisfied: matplotlib==3.7.2 in /usr/local/lib/python3.10/dist-packages (from d2l) (3.7.2)\n",
            "Requirement already satisfied: matplotlib-inline==0.1.6 in /usr/local/lib/python3.10/dist-packages (from d2l) (0.1.6)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from d2l) (2.31.0)\n",
            "Requirement already satisfied: pandas==2.0.3 in /usr/local/lib/python3.10/dist-packages (from d2l) (2.0.3)\n",
            "Requirement already satisfied: scipy==1.10.1 in /usr/local/lib/python3.10/dist-packages (from d2l) (1.10.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l) (6.5.5)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l) (5.6.0)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l) (6.5.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l) (5.5.6)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l) (7.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (10.4.0)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (2.8.2)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.10/dist-packages (from matplotlib-inline==0.1.6->d2l) (5.7.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->d2l) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->d2l) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.7.2->d2l) (1.16.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (6.3.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.6.9)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.0.13)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter==1.0.0->d2l) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter==1.0.0->d2l) (2.18.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.4)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (3.1.4)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (2.1.5)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.10.0)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (5.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (1.3.0)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (23.1.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (0.21.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (1.1.0)\n",
            "Requirement already satisfied: qtpy>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from qtconsole->jupyter==1.0.0->d2l) (2.4.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (71.0.4)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (4.9.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter==1.0.0->d2l) (4.3.6)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (0.2.4)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (4.23.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter==1.0.0->d2l) (0.2.13)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->jupyter==1.0.0->d2l) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter==1.0.0->d2l) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->d2l) (2.6)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter==1.0.0->d2l) (0.5.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (0.20.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaHKOh8hQxIa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(d2l.Classifier):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LazyConv2d(96, kernel_size=11, stride=4, padding=1),\n",
        "            nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.LazyConv2d(256, kernel_size=5, padding=2), nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.LazyConv2d(256, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(),\n",
        "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(p=0.5),\n",
        "            nn.LazyLinear(4096), nn.ReLU(),nn.Dropout(p=0.5),\n",
        "            nn.LazyLinear(num_classes))\n",
        "        self.net.apply(d2l.init_cnn)"
      ],
      "metadata": {
        "id": "v-aBRFQGQ1wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AlexNet().layer_summary((1, 1, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk8eSRw0Q3xm",
        "outputId": "765fc2d2-769d-4b4c-8878-ca4e4bd7156b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conv2d output shape:\t torch.Size([1, 96, 54, 54])\n",
            "ReLU output shape:\t torch.Size([1, 96, 54, 54])\n",
            "MaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\n",
            "Conv2d output shape:\t torch.Size([1, 256, 26, 26])\n",
            "ReLU output shape:\t torch.Size([1, 256, 26, 26])\n",
            "MaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\n",
            "Conv2d output shape:\t torch.Size([1, 384, 12, 12])\n",
            "ReLU output shape:\t torch.Size([1, 384, 12, 12])\n",
            "Conv2d output shape:\t torch.Size([1, 384, 12, 12])\n",
            "ReLU output shape:\t torch.Size([1, 384, 12, 12])\n",
            "Conv2d output shape:\t torch.Size([1, 256, 12, 12])\n",
            "ReLU output shape:\t torch.Size([1, 256, 12, 12])\n",
            "MaxPool2d output shape:\t torch.Size([1, 256, 5, 5])\n",
            "Flatten output shape:\t torch.Size([1, 6400])\n",
            "Linear output shape:\t torch.Size([1, 4096])\n",
            "ReLU output shape:\t torch.Size([1, 4096])\n",
            "Dropout output shape:\t torch.Size([1, 4096])\n",
            "Linear output shape:\t torch.Size([1, 4096])\n",
            "ReLU output shape:\t torch.Size([1, 4096])\n",
            "Dropout output shape:\t torch.Size([1, 4096])\n",
            "Linear output shape:\t torch.Size([1, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Let's delve into a comprehensive analysis of AlexNet, addressing each of your queries in detail. We'll cover computational properties, memory footprint, computational costs, memory bandwidth impacts, chip design trade-offs, benchmarking practices, training behaviors compared to LeNet, model simplifications, and more.\n",
        "\n",
        "1. Computational Properties of AlexNet\n",
        "AlexNet is a pioneering convolutional neural network (CNN) architecture introduced by Alex Krizhevsky et al. in 2012. It significantly advanced the field of deep learning, particularly in image classification tasks. Here's a breakdown of its computational properties:\n",
        "\n",
        "Architecture Overview:\n",
        "\n",
        "Layers: 8 layers (5 convolutional layers followed by 3 fully connected layers).\n",
        "Parameters: Approximately 60 million parameters.\n",
        "Activation Functions: ReLU (Rectified Linear Unit) activations.\n",
        "Regularization: Dropout and data augmentation techniques.\n",
        "Key Features:\n",
        "\n",
        "Deep Architecture: Allowed for learning hierarchical feature representations.\n",
        "Use of GPUs: Leveraged GPU parallelism for training efficiency.\n",
        "Local Response Normalization (LRN): Applied after certain layers to aid generalization.\n",
        "2. Memory Footprint for Convolutions and Fully Connected Layers\n",
        "a. Memory Footprint Calculation\n",
        "Memory footprint refers to the amount of memory required to store the parameters and intermediate activations during the forward and backward passes.\n",
        "\n",
        "Convolutional Layers:\n",
        "\n",
        "Parameters: Number of filters × (filter height × filter width × input channels) + biases.\n",
        "Activations: Output feature maps × spatial dimensions.\n",
        "Fully Connected Layers:\n",
        "\n",
        "Parameters: Number of neurons in the layer × number of inputs + biases.\n",
        "Activations: Number of neurons.\n",
        "Example Calculation for AlexNet:\n",
        "\n",
        "Assuming input images of size 224x224x3 (standard for AlexNet):\n",
        "\n",
        "Convolutional Layers:\n",
        "Conv1: 96 filters of size 11x11x3 → Parameters: 96 × (11×11×3) + 96 ≈ 34,944.\n",
        "Similar calculations apply to other convolutional layers.\n",
        "Fully Connected Layers:\n",
        "FC6: 4096 neurons × (flattened input from previous layer) + 4096 biases.\n",
        "This results in a significantly higher number of parameters compared to convolutional layers.\n",
        "b. Dominance: Fully Connected Layers\n",
        "In AlexNet, fully connected layers dominate the memory footprint due to their dense connectivity and large number of parameters. For instance, FC6 alone accounts for a substantial portion of the total parameters and memory usage.\n",
        "\n",
        "3. Computational Cost for Convolutions and Fully Connected Layers\n",
        "a. Computational Cost Metrics\n",
        "FLOPs (Floating Point Operations): A common metric to measure computational cost.\n",
        "Convolutional Layers:\n",
        "\n",
        "FLOPs per layer: Number of operations = 2 × (Number of filters) × (Filter height × Filter width × Input channels) × (Output feature map height × Output feature map width).\n",
        "Example: For Conv1 in AlexNet: 2 × 96 × (11×11×3) × (55×55) ≈ 1.08 billion FLOPs.\n",
        "Fully Connected Layers:\n",
        "\n",
        "FLOPs per layer: 2 × (Number of input neurons) × (Number of output neurons).\n",
        "Example: FC6: 2 × (flattened input size) × 4096 ≈ several billion FLOPs.\n",
        "b. Dominance: Fully Connected Layers\n",
        "Fully connected layers typically require more FLOPs compared to convolutional layers, especially in deep networks like AlexNet. This is due to the dense connections and large number of parameters, leading to higher computational demands.\n",
        "\n",
        "4. Impact of Memory (Bandwidth, Latency, Size) on Computation\n",
        "a. Memory Characteristics:\n",
        "Read and Write Bandwidth: The rate at which data can be read from or written to memory.\n",
        "Latency: Time delay between a request for data and the delivery of data.\n",
        "Memory Size: Total capacity available for storing data and parameters.\n",
        "b. Effects on Computation:\n",
        "Bandwidth Bottlenecks:\n",
        "\n",
        "High computational demands require substantial data movement.\n",
        "Limited bandwidth can lead to data stalls, reducing overall throughput.\n",
        "Latency Impacts:\n",
        "\n",
        "High latency can delay data availability, impacting the efficiency of parallel computations.\n",
        "Memory Size Constraints:\n",
        "\n",
        "Insufficient memory can force frequent data swapping between faster (e.g., GPU) and slower (e.g., CPU) memories, increasing computation time.\n",
        "c. Differences for Training and Inference:\n",
        "Training:\n",
        "\n",
        "Higher Memory Usage: Due to storage of intermediate activations for backpropagation.\n",
        "Increased Bandwidth Demand: Frequent data reads/writes for gradient calculations.\n",
        "Latency Sensitivity: More pronounced as training requires sequential computations (forward and backward passes).\n",
        "Inference:\n",
        "\n",
        "Lower Memory Requirements: Only forward pass activations are needed.\n",
        "Bandwidth Less Critical: Reduced data movement compared to training.\n",
        "Latency Less Sensitive: Especially in batch processing scenarios.\n",
        "5. Chip Design Trade-offs: Balancing Computation and Memory Bandwidth\n",
        "As a chip designer aiming to optimize for both computation and memory bandwidth while considering power and area constraints, the following trade-offs are essential:\n",
        "\n",
        "a. Optimization Strategies:\n",
        "Compute vs. Memory:\n",
        "\n",
        "Higher Compute Capability:\n",
        "Increases parallel processing units (e.g., more ALUs).\n",
        "Trade-Off: Higher power consumption and larger chip area.\n",
        "Enhanced Memory Bandwidth:\n",
        "Wider memory buses or higher frequency memory.\n",
        "Trade-Off: More pins, increased control logic, larger chip area.\n",
        "Memory Hierarchy Design:\n",
        "\n",
        "Implementing caches (L1, L2) to reduce bandwidth demands.\n",
        "Utilizing on-chip SRAM to store frequently accessed data, minimizing off-chip memory accesses.\n",
        "Data Reuse and Local Storage:\n",
        "\n",
        "Designing processing elements that can reuse data from local storage reduces the need for external memory bandwidth.\n",
        "Power Management:\n",
        "\n",
        "Dynamic voltage and frequency scaling (DVFS) to adjust power based on computational load.\n",
        "Power gating unused parts of the chip to save energy.\n",
        "b. Balancing Act:\n",
        "Performance Requirements: Prioritize based on whether the application demands high throughput (e.g., real-time inference) or high flexibility.\n",
        "Area Constraints: Optimize the layout to maximize parallelism without excessively increasing chip size.\n",
        "Power Budget: Ensure that the design adheres to power consumption limits, especially for mobile or embedded applications.\n",
        "c. Example Optimization:\n",
        "ASIC Design for AlexNet:\n",
        "Parallel Convolution Units: To handle multiple filters simultaneously.\n",
        "High-Bandwidth Memory Interfaces: To supply data quickly to convolution units.\n",
        "On-Chip Memory Buffers: To store frequently accessed weights and activations, reducing external memory accesses.\n",
        "6. Decline in Reporting Performance Benchmarks on AlexNet\n",
        "Reasons Engineers No Longer Report Performance Benchmarks on AlexNet:\n",
        "\n",
        "Advancements in Architecture:\n",
        "\n",
        "Newer architectures like VGG, ResNet, Inception, and EfficientNet offer better performance and efficiency.\n",
        "AlexNet is considered outdated in comparison to these models.\n",
        "Dataset Evolution:\n",
        "\n",
        "AlexNet was primarily benchmarked on ImageNet, but newer datasets with higher complexity and different characteristics are now standard.\n",
        "Hardware Improvements:\n",
        "\n",
        "Modern GPUs and specialized accelerators have evolved, making AlexNet benchmarks less relevant as they no longer reflect current hardware capabilities.\n",
        "Research Focus Shift:\n",
        "\n",
        "The research community focuses on more challenging tasks, such as object detection, segmentation, and tasks requiring greater accuracy and efficiency.\n",
        "Optimization and Efficiency:\n",
        "\n",
        "Contemporary models emphasize not just accuracy but also efficiency in terms of parameter count, FLOPs, and memory usage, areas where AlexNet is less competitive.\n",
        "7. Increasing Epochs: AlexNet vs. LeNet\n",
        "a. Training AlexNet for More Epochs\n",
        "Behavior:\n",
        "AlexNet: With its deeper architecture and larger number of parameters, increasing epochs can lead to better convergence and potentially higher accuracy up to a point. However, it also increases the risk of overfitting if not properly regularized.\n",
        "Compared to LeNet:\n",
        "LeNet: A simpler architecture with fewer parameters, suitable for simpler tasks like digit recognition.\n",
        "Impact of More Epochs:\n",
        "LeNet: May quickly converge to optimal performance with fewer epochs; additional epochs offer diminishing returns.\n",
        "AlexNet: Requires more epochs to fully train due to its complexity but benefits more from extended training to capture intricate patterns.\n",
        "b. Reasons for Differences:\n",
        "Model Complexity:\n",
        "\n",
        "AlexNet's deeper and wider architecture can model more complex functions, benefiting from additional training.\n",
        "LeNet's simplicity may not leverage the benefits of extended training to the same extent.\n",
        "Dataset Complexity:\n",
        "\n",
        "AlexNet is typically trained on more complex datasets (e.g., ImageNet), which require more training epochs to learn diverse features.\n",
        "LeNet is often applied to simpler datasets (e.g., MNIST), which are easier to learn quickly.\n",
        "Overfitting Risks:\n",
        "\n",
        "AlexNet is more prone to overfitting with more epochs due to its high capacity, necessitating careful regularization.\n",
        "LeNet's lower capacity reduces the risk of overfitting with additional epochs.\n",
        "8. Complexity of AlexNet for Fashion-MNIST\n",
        "Issue: AlexNet may be too complex for the Fashion-MNIST dataset, particularly due to the low resolution of the initial images.\n",
        "\n",
        "a. Reasons:\n",
        "High Parameter Count:\n",
        "\n",
        "Fashion-MNIST images are 28x28 grayscale images, much smaller and simpler than ImageNet images (224x224 RGB).\n",
        "AlexNet's large number of parameters can lead to overfitting on such a simple dataset.\n",
        "Redundant Computations:\n",
        "\n",
        "The depth and complexity of AlexNet are unnecessary for the relatively low variability in Fashion-MNIST.\n",
        "Inefficient Use of Resources:\n",
        "\n",
        "Large fully connected layers consume excessive memory and computational resources without proportional benefits.\n",
        "b. Consequences:\n",
        "Longer Training Times: Due to increased computational demands.\n",
        "Potential Overfitting: The model may not generalize well to unseen data.\n",
        "Inefficient Performance: Higher latency and lower throughput without accuracy gains.\n",
        "9. Simplifying AlexNet for Faster Training on Fashion-MNIST\n",
        "To optimize training speed while maintaining accuracy on Fashion-MNIST, consider simplifying the model:\n",
        "\n",
        "a. Strategies:\n",
        "Reduce Depth:\n",
        "\n",
        "Decrease the number of convolutional and fully connected layers.\n",
        "Decrease Width:\n",
        "\n",
        "Use fewer filters in each convolutional layer.\n",
        "Replace Fully Connected Layers:\n",
        "\n",
        "Use global average pooling instead of dense layers to reduce parameters.\n",
        "Input Image Size:\n",
        "\n",
        "Adjust the input size to match the dataset (e.g., 32x32 instead of 224x224) to reduce computational load.\n",
        "Use Batch Normalization:\n",
        "\n",
        "Stabilizes and accelerates training, allowing for higher learning rates.\n",
        "Apply Regularization:\n",
        "\n",
        "Techniques like dropout can prevent overfitting, allowing for smaller models."
      ],
      "metadata": {
        "id": "LmafYpzejsVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimplifiedAlexNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimplifiedAlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # Adjusted for grayscale\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(64 * 7 * 7, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256, 10),  # Assuming 10 classes for Fashion-MNIST\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), 64 * 7 * 7)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ZOB8pdJtjzVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c. Benefits:\n",
        "Reduced Parameters: Lower memory footprint and faster computations.\n",
        "Faster Training: Due to fewer layers and parameters.\n",
        "Maintained Accuracy: Adequate capacity for Fashion-MNIST's complexity.\n",
        "10. Designing a Better Model for Direct Image Processing\n",
        "To design a model that works directly on images (assuming high-resolution images), consider the following enhancements over AlexNet:\n",
        "\n",
        "a. Modern Architectural Improvements:\n",
        "Residual Connections (ResNet):\n",
        "\n",
        "Helps in training deeper networks by mitigating vanishing gradients.\n",
        "Inception Modules:\n",
        "\n",
        "Allows for multi-scale feature extraction within the same layer.\n",
        "Depthwise Separable Convolutions (MobileNet):\n",
        "\n",
        "Reduces computational cost while maintaining performance.\n",
        "Batch Normalization:\n",
        "\n",
        "Stabilizes and accelerates training.\n",
        "Efficient Activation Functions:\n",
        "\n",
        "Leaky ReLU or ELU can offer better gradient flow."
      ],
      "metadata": {
        "id": "K_0uGM_wj0m2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ImprovedCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImprovedCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256 * 6 * 6, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 10),  # Adjust as per the number of classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), 256 * 6 * 6)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "9Q4KUxEEj3fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ImprovedCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImprovedCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "            \n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "            \n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256 * 6 * 6, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 10),  # Adjust as per the number of classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), 256 * 6 * 6)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "mVuNanYhj5hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EnhancedLeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedLeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(64 * 4 * 4, 256)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "frA9aqhbj9sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Making AlexNet Overfit and Breaking Training\n",
        "a. Overfitting AlexNet:\n",
        "Overfitting occurs when a model learns the training data too well, including its noise and outliers, leading to poor generalization on unseen data.\n",
        "\n",
        "How to Make AlexNet Overfit:\n",
        "\n",
        "Increase Model Capacity:\n",
        "\n",
        "Utilize all layers without regularization.\n",
        "Maintain high parameter counts.\n",
        "Reduce Training Data:\n",
        "\n",
        "Limit the number of training samples to make the model memorize the data.\n",
        "Remove Regularization:\n",
        "\n",
        "Eliminate dropout layers and weight decay.\n",
        "Train for Excessive Epochs:\n",
        "\n",
        "Allow the model to train beyond the point of optimal generalization.\n",
        "b. Breaking Training to Prevent Overfitting:\n",
        "To prevent or break overfitting, certain features or practices need to be altered or removed:\n",
        "\n",
        "Introduce Regularization:\n",
        "\n",
        "Dropout: Randomly deactivates neurons during training.\n",
        "Weight Decay (L2 Regularization): Penalizes large weights.\n",
        "Data Augmentation:\n",
        "\n",
        "Increases the diversity of training data, forcing the model to learn more generalized features.\n",
        "Early Stopping:\n",
        "\n",
        "Halt training when validation performance stops improving.\n",
        "Reduce Model Complexity:\n",
        "\n",
        "Decrease the number of layers or parameters.\n",
        "Implement Batch Normalization:\n",
        "\n",
        "Stabilizes learning and can have a regularizing effect.\n",
        "c. Specific Feature to Remove or Change:\n",
        "Remove Dropout Layers:\n",
        "\n",
        "To intentionally overfit, eliminating dropout reduces regularization, allowing the model to memorize training data."
      ],
      "metadata": {
        "id": "gf47A2wTkArU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNetOverfit(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AlexNetOverfit, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # ... (same as original AlexNet)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # nn.Dropout(),  # Removed Dropout\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # nn.Dropout(),  # Removed Dropout\n",
        "            nn.Linear(4096, 1000),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), 256 * 6 * 6)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "rj4a6dxJkCv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AlexNet(lr=0.01)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "0RsPix6CQ6BH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73a74146-5319-4bff-a4d3-3ec16da46aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:02<00:00, 9786852.68it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ../data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 204421.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 3735794.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ../data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 13832336.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vgg_block(num_convs, out_channels):\n",
        "    layers = []\n",
        "    for _ in range(num_convs):\n",
        "        layers.append(nn.LazyConv2d(out_channels, kernel_size=3, padding=1))\n",
        "        layers.append(nn.ReLU())\n",
        "    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
        "    return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "1cLWoLl_RLzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG(d2l.Classifier):\n",
        "    def __init__(self, arch, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        conv_blks = []\n",
        "        for (num_convs, out_channels) in arch:\n",
        "            conv_blks.append(vgg_block(num_convs, out_channels))\n",
        "        self.net = nn.Sequential(\n",
        "            *conv_blks, nn.Flatten(),\n",
        "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.LazyLinear(num_classes))\n",
        "        self.net.apply(d2l.init_cnn)"
      ],
      "metadata": {
        "id": "5pclssjCRN3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VGG(arch=((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))).layer_summary(\n",
        "    (1, 1, 224, 224))"
      ],
      "metadata": {
        "id": "DN8RiCbrRa8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VGG(arch=((1, 16), (1, 32), (2, 64), (2, 128), (2, 128)), lr=0.01)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "cEP3rQJlRbgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nin_block(out_channels, kernel_size, strides, padding):\n",
        "    return nn.Sequential(\n",
        "        nn.LazyConv2d(out_channels, kernel_size, strides, padding), nn.ReLU(),\n",
        "        nn.LazyConv2d(out_channels, kernel_size=1), nn.ReLU(),\n",
        "        nn.LazyConv2d(out_channels, kernel_size=1), nn.ReLU())"
      ],
      "metadata": {
        "id": "TLuTNZEbRmFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NiN().layer_summary((1, 1, 224, 224))"
      ],
      "metadata": {
        "id": "MCVZje5gRnSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NiN(lr=0.05)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "1HpeBem5RpnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Inception(nn.Module):\n",
        "    # c1--c4 are the number of output channels for each branch\n",
        "    def __init__(self, c1, c2, c3, c4, **kwargs):\n",
        "        super(Inception, self).__init__(**kwargs)\n",
        "        # Branch 1\n",
        "        self.b1_1 = nn.LazyConv2d(c1, kernel_size=1)\n",
        "        # Branch 2\n",
        "        self.b2_1 = nn.LazyConv2d(c2[0], kernel_size=1)\n",
        "        self.b2_2 = nn.LazyConv2d(c2[1], kernel_size=3, padding=1)\n",
        "        # Branch 3\n",
        "        self.b3_1 = nn.LazyConv2d(c3[0], kernel_size=1)\n",
        "        self.b3_2 = nn.LazyConv2d(c3[1], kernel_size=5, padding=2)\n",
        "        # Branch 4\n",
        "        self.b4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "        self.b4_2 = nn.LazyConv2d(c4, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b1 = F.relu(self.b1_1(x))\n",
        "        b2 = F.relu(self.b2_2(F.relu(self.b2_1(x))))\n",
        "        b3 = F.relu(self.b3_2(F.relu(self.b3_1(x))))\n",
        "        b4 = F.relu(self.b4_2(self.b4_1(x)))\n",
        "        return torch.cat((b1, b2, b3, b4), dim=1)"
      ],
      "metadata": {
        "id": "7sCZA4jFRq-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GoogleNet(d2l.Classifier):\n",
        "    def b1(self):\n",
        "        return nn.Sequential(\n",
        "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
      ],
      "metadata": {
        "id": "EgwXGnYpRuSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(GoogleNet)\n",
        "def b2(self):\n",
        "    return nn.Sequential(\n",
        "        nn.LazyConv2d(64, kernel_size=1), nn.ReLU(),\n",
        "        nn.LazyConv2d(192, kernel_size=3, padding=1), nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
      ],
      "metadata": {
        "id": "9g587PmuRv7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(GoogleNet)\n",
        "def b3(self):\n",
        "    return nn.Sequential(Inception(64, (96, 128), (16, 32), 32),\n",
        "                         Inception(128, (128, 192), (32, 96), 64),\n",
        "                         nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
      ],
      "metadata": {
        "id": "cjaa0hzBRxRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(GoogleNet)\n",
        "def b4(self):\n",
        "    return nn.Sequential(Inception(192, (96, 208), (16, 48), 64),\n",
        "                         Inception(160, (112, 224), (24, 64), 64),\n",
        "                         Inception(128, (128, 256), (24, 64), 64),\n",
        "                         Inception(112, (144, 288), (32, 64), 64),\n",
        "                         Inception(256, (160, 320), (32, 128), 128),\n",
        "                         nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
      ],
      "metadata": {
        "id": "-qfpTM8qRzO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(GoogleNet)\n",
        "def b5(self):\n",
        "    return nn.Sequential(Inception(256, (160, 320), (32, 128), 128),\n",
        "                         Inception(384, (192, 384), (48, 128), 128),\n",
        "                         nn.AdaptiveAvgPool2d((1,1)), nn.Flatten())"
      ],
      "metadata": {
        "id": "Bis2NIXVR3YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(GoogleNet)\n",
        "def __init__(self, lr=0.1, num_classes=10):\n",
        "    super(GoogleNet, self).__init__()\n",
        "    self.save_hyperparameters()\n",
        "    self.net = nn.Sequential(self.b1(), self.b2(), self.b3(), self.b4(),\n",
        "                             self.b5(), nn.LazyLinear(num_classes))\n",
        "    self.net.apply(d2l.init_cnn)"
      ],
      "metadata": {
        "id": "X8_NikwIR6ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GoogleNet().layer_summary((1, 1, 96, 96))"
      ],
      "metadata": {
        "id": "o4WgIVp-R7Nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GoogleNet(lr=0.01)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "Y4PtmLrjR9b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
        "    # Use is_grad_enabled to determine whether we are in training mode\n",
        "    if not torch.is_grad_enabled():\n",
        "        # In prediction mode, use mean and variance obtained by moving average\n",
        "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
        "    else:\n",
        "        assert len(X.shape) in (2, 4)\n",
        "        if len(X.shape) == 2:\n",
        "            # When using a fully connected layer, calculate the mean and\n",
        "            # variance on the feature dimension\n",
        "            mean = X.mean(dim=0)\n",
        "            var = ((X - mean) ** 2).mean(dim=0)\n",
        "        else:\n",
        "            # When using a two-dimensional convolutional layer, calculate the\n",
        "            # mean and variance on the channel dimension (axis=1). Here we\n",
        "            # need to maintain the shape of X, so that the broadcasting\n",
        "            # operation can be carried out later\n",
        "            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
        "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
        "        # In training mode, the current mean and variance are used\n",
        "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
        "        # Update the mean and variance using moving average\n",
        "        moving_mean = (1.0 - momentum) * moving_mean + momentum * mean\n",
        "        moving_var = (1.0 - momentum) * moving_var + momentum * var\n",
        "    Y = gamma * X_hat + beta  # Scale and shift\n",
        "    return Y, moving_mean.data, moving_var.data"
      ],
      "metadata": {
        "id": "Bg9PFKAlR_hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNorm(nn.Module):\n",
        "    # num_features: the number of outputs for a fully connected layer or the\n",
        "    # number of output channels for a convolutional layer. num_dims: 2 for a\n",
        "    # fully connected layer and 4 for a convolutional layer\n",
        "    def __init__(self, num_features, num_dims):\n",
        "        super().__init__()\n",
        "        if num_dims == 2:\n",
        "            shape = (1, num_features)\n",
        "        else:\n",
        "            shape = (1, num_features, 1, 1)\n",
        "        # The scale parameter and the shift parameter (model parameters) are\n",
        "        # initialized to 1 and 0, respectively\n",
        "        self.gamma = nn.Parameter(torch.ones(shape))\n",
        "        self.beta = nn.Parameter(torch.zeros(shape))\n",
        "        # The variables that are not model parameters are initialized to 0 and\n",
        "        # 1\n",
        "        self.moving_mean = torch.zeros(shape)\n",
        "        self.moving_var = torch.ones(shape)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # If X is not on the main memory, copy moving_mean and moving_var to\n",
        "        # the device where X is located\n",
        "        if self.moving_mean.device != X.device:\n",
        "            self.moving_mean = self.moving_mean.to(X.device)\n",
        "            self.moving_var = self.moving_var.to(X.device)\n",
        "        # Save the updated moving_mean and moving_var\n",
        "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
        "            X, self.gamma, self.beta, self.moving_mean,\n",
        "            self.moving_var, eps=1e-5, momentum=0.1)\n",
        "        return Y"
      ],
      "metadata": {
        "id": "m36jmCGzSDEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BNLeNetScratch(d2l.Classifier):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LazyConv2d(6, kernel_size=5), BatchNorm(6, num_dims=4),\n",
        "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.LazyConv2d(16, kernel_size=5), BatchNorm(16, num_dims=4),\n",
        "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(), nn.LazyLinear(120),\n",
        "            BatchNorm(120, num_dims=2), nn.Sigmoid(), nn.LazyLinear(84),\n",
        "            BatchNorm(84, num_dims=2), nn.Sigmoid(),\n",
        "            nn.LazyLinear(num_classes))"
      ],
      "metadata": {
        "id": "aCL3BbKHSEcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128)\n",
        "model = BNLeNetScratch(lr=0.1)\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "GotK2XMsSJLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.net[1].gamma.reshape((-1,)), model.net[1].beta.reshape((-1,))"
      ],
      "metadata": {
        "id": "6dDxjSuUSKio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BNLeNet(d2l.Classifier):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LazyConv2d(6, kernel_size=5), nn.LazyBatchNorm2d(),\n",
        "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.LazyConv2d(16, kernel_size=5), nn.LazyBatchNorm2d(),\n",
        "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(), nn.LazyLinear(120), nn.LazyBatchNorm1d(),\n",
        "            nn.Sigmoid(), nn.LazyLinear(84), nn.LazyBatchNorm1d(),\n",
        "            nn.Sigmoid(), nn.LazyLinear(num_classes))"
      ],
      "metadata": {
        "id": "sp4eelWCSK3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128)\n",
        "model = BNLeNet(lr=0.1)\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "tOo4nK4sSQFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Residual(nn.Module):\n",
        "    \"\"\"The Residual block of ResNet models.\"\"\"\n",
        "    def __init__(self, num_channels, use_1x1conv=False, strides=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1,\n",
        "                                   stride=strides)\n",
        "        self.conv2 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1)\n",
        "        if use_1x1conv:\n",
        "            self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
        "                                       stride=strides)\n",
        "        else:\n",
        "            self.conv3 = None\n",
        "        self.bn1 = nn.LazyBatchNorm2d()\n",
        "        self.bn2 = nn.LazyBatchNorm2d()\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = F.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "        if self.conv3:\n",
        "            X = self.conv3(X)\n",
        "        Y += X\n",
        "        return F.relu(Y)"
      ],
      "metadata": {
        "id": "OSD9tzwlSTtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blk = Residual(3)\n",
        "X = torch.randn(4, 3, 6, 6)\n",
        "blk(X).shape"
      ],
      "metadata": {
        "id": "W1CqMOTASVJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blk = Residual(6, use_1x1conv=True, strides=2)\n",
        "blk(X).shape"
      ],
      "metadata": {
        "id": "1ff7kiedSY7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(d2l.Classifier):\n",
        "    def b1(self):\n",
        "        return nn.Sequential(\n",
        "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
      ],
      "metadata": {
        "id": "1mjfa13USc17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(ResNet)\n",
        "def block(self, num_residuals, num_channels, first_block=False):\n",
        "    blk = []\n",
        "    for i in range(num_residuals):\n",
        "        if i == 0 and not first_block:\n",
        "            blk.append(Residual(num_channels, use_1x1conv=True, strides=2))\n",
        "        else:\n",
        "            blk.append(Residual(num_channels))\n",
        "    return nn.Sequential(*blk)"
      ],
      "metadata": {
        "id": "riuPgoy7Se0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(ResNet)\n",
        "def __init__(self, arch, lr=0.1, num_classes=10):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.save_hyperparameters()\n",
        "    self.net = nn.Sequential(self.b1())\n",
        "    for i, b in enumerate(arch):\n",
        "        self.net.add_module(f'b{i+2}', self.block(*b, first_block=(i==0)))\n",
        "    self.net.add_module('last', nn.Sequential(\n",
        "        nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
        "        nn.LazyLinear(num_classes)))\n",
        "    self.net.apply(d2l.init_cnn)"
      ],
      "metadata": {
        "id": "S0yDAw4ZSgyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet18(ResNet):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__(((2, 64), (2, 128), (2, 256), (2, 512)),\n",
        "                       lr, num_classes)\n",
        "\n",
        "ResNet18().layer_summary((1, 1, 96, 96))"
      ],
      "metadata": {
        "id": "NvPYZ97pShUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet18(lr=0.01)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "EI1V7nDuSivI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNeXtBlock(nn.Module):\n",
        "    \"\"\"The ResNeXt block.\"\"\"\n",
        "    def __init__(self, num_channels, groups, bot_mul, use_1x1conv=False,\n",
        "                 strides=1):\n",
        "        super().__init__()\n",
        "        bot_channels = int(round(num_channels * bot_mul))\n",
        "        self.conv1 = nn.LazyConv2d(bot_channels, kernel_size=1, stride=1)\n",
        "        self.conv2 = nn.LazyConv2d(bot_channels, kernel_size=3,\n",
        "                                   stride=strides, padding=1,\n",
        "                                   groups=bot_channels//groups)\n",
        "        self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1, stride=1)\n",
        "        self.bn1 = nn.LazyBatchNorm2d()\n",
        "        self.bn2 = nn.LazyBatchNorm2d()\n",
        "        self.bn3 = nn.LazyBatchNorm2d()\n",
        "        if use_1x1conv:\n",
        "            self.conv4 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
        "                                       stride=strides)\n",
        "            self.bn4 = nn.LazyBatchNorm2d()\n",
        "        else:\n",
        "            self.conv4 = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = F.relu(self.bn1(self.conv1(X)))\n",
        "        Y = F.relu(self.bn2(self.conv2(Y)))\n",
        "        Y = self.bn3(self.conv3(Y))\n",
        "        if self.conv4:\n",
        "            X = self.bn4(self.conv4(X))\n",
        "        return F.relu(Y + X)"
      ],
      "metadata": {
        "id": "Y_8CdT17Skln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blk = ResNeXtBlock(32, 16, 1)\n",
        "X = torch.randn(4, 32, 96, 96)\n",
        "blk(X).shape"
      ],
      "metadata": {
        "id": "rm3h_HyGSmE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(num_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "        nn.LazyConv2d(num_channels, kernel_size=3, padding=1))"
      ],
      "metadata": {
        "id": "ENRtyiZaSq6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, num_convs, num_channels):\n",
        "        super(DenseBlock, self).__init__()\n",
        "        layer = []\n",
        "        for i in range(num_convs):\n",
        "            layer.append(conv_block(num_channels))\n",
        "        self.net = nn.Sequential(*layer)\n",
        "\n",
        "    def forward(self, X):\n",
        "        for blk in self.net:\n",
        "            Y = blk(X)\n",
        "            # Concatenate input and output of each block along the channels\n",
        "            X = torch.cat((X, Y), dim=1)\n",
        "        return X"
      ],
      "metadata": {
        "id": "fhJxU8PcSq8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blk = DenseBlock(2, 10)\n",
        "X = torch.randn(4, 3, 8, 8)\n",
        "Y = blk(X)\n",
        "Y.shape"
      ],
      "metadata": {
        "id": "ndtkOR8iSwbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transition_block(num_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "        nn.LazyConv2d(num_channels, kernel_size=1),\n",
        "        nn.AvgPool2d(kernel_size=2, stride=2))"
      ],
      "metadata": {
        "id": "nnw9vV7gSw2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blk = transition_block(10)\n",
        "blk(Y).shape"
      ],
      "metadata": {
        "id": "8NFO681wSyOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseNet(d2l.Classifier):\n",
        "    def b1(self):\n",
        "        return nn.Sequential(\n",
        "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
      ],
      "metadata": {
        "id": "_Q2UsF2XS14_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(DenseNet)\n",
        "def __init__(self, num_channels=64, growth_rate=32, arch=(4, 4, 4, 4),\n",
        "             lr=0.1, num_classes=10):\n",
        "    super(DenseNet, self).__init__()\n",
        "    self.save_hyperparameters()\n",
        "    self.net = nn.Sequential(self.b1())\n",
        "    for i, num_convs in enumerate(arch):\n",
        "        self.net.add_module(f'dense_blk{i+1}', DenseBlock(num_convs,\n",
        "                                                          growth_rate))\n",
        "        # The number of output channels in the previous dense block\n",
        "        num_channels += num_convs * growth_rate\n",
        "        # A transition layer that halves the number of channels is added\n",
        "        # between the dense blocks\n",
        "        if i != len(arch) - 1:\n",
        "            num_channels //= 2\n",
        "            self.net.add_module(f'tran_blk{i+1}', transition_block(\n",
        "                num_channels))\n",
        "    self.net.add_module('last', nn.Sequential(\n",
        "        nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "        nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
        "        nn.LazyLinear(num_classes)))\n",
        "    self.net.apply(d2l.init_cnn)"
      ],
      "metadata": {
        "id": "Qx8qHkxHS3ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DenseNet(lr=0.01)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "lZosoEiuS4pH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from d2l import torch as d2l"
      ],
      "metadata": {
        "id": "TCOhDzs9S8i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AnyNet(d2l.Classifier):\n",
        "    def stem(self, num_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.LazyConv2d(num_channels, kernel_size=3, stride=2, padding=1),\n",
        "            nn.LazyBatchNorm2d(), nn.ReLU())"
      ],
      "metadata": {
        "id": "YveCgMm_S-CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(AnyNet)\n",
        "def stage(self, depth, num_channels, groups, bot_mul):\n",
        "    blk = []\n",
        "    for i in range(depth):\n",
        "        if i == 0:\n",
        "            blk.append(d2l.ResNeXtBlock(num_channels, groups, bot_mul,\n",
        "                use_1x1conv=True, strides=2))\n",
        "        else:\n",
        "            blk.append(d2l.ResNeXtBlock(num_channels, groups, bot_mul))\n",
        "    return nn.Sequential(*blk)"
      ],
      "metadata": {
        "id": "JHi666tWS_H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(AnyNet)\n",
        "def __init__(self, arch, stem_channels, lr=0.1, num_classes=10):\n",
        "    super(AnyNet, self).__init__()\n",
        "    self.save_hyperparameters()\n",
        "    self.net = nn.Sequential(self.stem(stem_channels))\n",
        "    for i, s in enumerate(arch):\n",
        "        self.net.add_module(f'stage{i+1}', self.stage(*s))\n",
        "    self.net.add_module('head', nn.Sequential(\n",
        "        nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
        "        nn.LazyLinear(num_classes)))\n",
        "    self.net.apply(d2l.init_cnn)"
      ],
      "metadata": {
        "id": "MDIq8pAFTBnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RegNetX32(AnyNet):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        stem_channels, groups, bot_mul = 32, 16, 1\n",
        "        depths, channels = (4, 6), (32, 80)\n",
        "        super().__init__(\n",
        "            ((depths[0], channels[0], groups, bot_mul),\n",
        "             (depths[1], channels[1], groups, bot_mul)),\n",
        "            stem_channels, lr, num_classes)"
      ],
      "metadata": {
        "id": "zgbfQuOwTDI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RegNetX32().layer_summary((1, 1, 96, 96))"
      ],
      "metadata": {
        "id": "Podi6_FzTDlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RegNetX32(lr=0.05)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "5sKDYXiATEx-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}